{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.174729Z",
     "start_time": "2025-07-12T20:14:40.171037Z"
    }
   },
   "source": "# RAG system using cycling/endurance coaching and training research papers",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.184721Z",
     "start_time": "2025-07-12T20:14:40.182085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import uuid\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ],
   "id": "20671ee778690f7e",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.210449Z",
     "start_time": "2025-07-12T20:14:40.207614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#YOUR KEY HERE\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ],
   "id": "76ea39dac6f29c0e",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.235701Z",
     "start_time": "2025-07-12T20:14:40.231591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunking(directory_path, tokenizer, chunk_size, para_seperator=\" /n /n\", separator=\" \"):\n",
    "\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    documents = {}\n",
    "    all_chunks = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        print(filename)\n",
    "        base = os.path.basename(file_path)\n",
    "        sku = os.path.splitext(base)[0]\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            doc_id = str(uuid.uuid4())\n",
    "\n",
    "            paragraphs = re.split(para_seperator, text)\n",
    "\n",
    "            for paragraph in paragraphs:\n",
    "                words = paragraph.split(separator)\n",
    "                current_chunk_str = \"\"\n",
    "                chunk = []\n",
    "                for word in words:\n",
    "                    if current_chunk_str:\n",
    "                        new_chunk = current_chunk_str + separator + word\n",
    "                    else:\n",
    "                        new_chunk = current_chunk_str + word\n",
    "                    if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n",
    "                        current_chunk_str = new_chunk\n",
    "                    else:\n",
    "                        if current_chunk_str:\n",
    "                            chunk.append(current_chunk_str)\n",
    "                        current_chunk_str = word\n",
    "\n",
    "\n",
    "                if current_chunk_str:\n",
    "                    chunk.append(current_chunk_str)\n",
    "\n",
    "                for chunk in chunk:\n",
    "                    chunk_id = str(uuid.uuid4())\n",
    "                    all_chunks[chunk_id] = {\"text\": chunk, \"metadata\": {\"file_name\":sku}}\n",
    "        documents[doc_id] = all_chunks\n",
    "    return documents\n"
   ],
   "id": "1a1ba973421970c2",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.257716Z",
     "start_time": "2025-07-12T20:14:40.253700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def map_document_embeddings(documents, tokenizer, model):\n",
    "    mapped_document_db = {}\n",
    "    for id, dict_content in documents.items():\n",
    "        mapped_embeddings = {}\n",
    "        for content_id, text_content in dict_content.items():\n",
    "            text = text_content.get(\"text\")\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "            mapped_embeddings[content_id] = embeddings\n",
    "        mapped_document_db[id] = mapped_embeddings\n",
    "    return mapped_document_db\n"
   ],
   "id": "376a459c311b6656",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.282386Z",
     "start_time": "2025-07-12T20:14:40.277541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_information(query, top_k, mapped_document_db):\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    query_embeddings = model(**query_inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "    query_embeddings=query_embeddings.tolist()\n",
    "    #converting query embeddings to numpy array\n",
    "    query_embeddings=np.array(query_embeddings)\n",
    "\n",
    "    scores = {}\n",
    "    #Now calculating cosine similarity\n",
    "    for doc_id, chunk_dict in mapped_document_db.items():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "            #converting chunk embedding to numpy array for efficient mathematical operations\n",
    "            chunk_embeddings = np.array(chunk_embeddings)\n",
    "\n",
    "            #Normalizing chunk embeddings and query embeddings  to get cosine similarity score\n",
    "            normalized_query = np.linalg.norm(query_embeddings)\n",
    "            normalized_chunk = np.linalg.norm(chunk_embeddings)\n",
    "\n",
    "            if normalized_chunk == 0 or normalized_query == 0:\n",
    "            # this is being done to avoid division with zero which will give wrong results i.e infinity. Hence to avoid this we set score to 0\n",
    "                score == 0\n",
    "            else:\n",
    "            # Now calculating cosine similarity score\n",
    "                score = np.dot(chunk_embeddings, query_embeddings)/ (normalized_chunk * normalized_query)\n",
    "\n",
    "             #STORING SCORES WITH THE REFERENCE\n",
    "            scores[(doc_id, chunk_id )] = score\n",
    "\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "\n",
    "    top_results=[]\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        results = (doc_id, chunk_id, score)\n",
    "        top_results.append(results)\n",
    "    return top_results\n"
   ],
   "id": "78d9890b7f9b19ec",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.304934Z",
     "start_time": "2025-07-12T20:14:40.300595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_embeddings(query, tokenizer, model):\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    query_embeddings = model(**query_inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "    query_embeddings=query_embeddings.tolist()\n",
    "    return query_embeddings\n"
   ],
   "id": "eb32ac0277ef8d02",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.326248Z",
     "start_time": "2025-07-12T20:14:40.323041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_cosine_similarity_score(query_embeddings, chunk_embeddings):\n",
    "        normalized_query = np.linalg.norm(query_embeddings)\n",
    "        normalized_chunk = np.linalg.norm(chunk_embeddings)\n",
    "        if normalized_chunk == 0 or normalized_query == 0:\n",
    "            score == 0\n",
    "        else:\n",
    "            score = np.dot(chunk_embeddings, query_embeddings)/ (normalized_chunk * normalized_query)\n",
    "        return score\n"
   ],
   "id": "3cb9eb1ad23627b2",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.350740Z",
     "start_time": "2025-07-12T20:14:40.347313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k):\n",
    "    scores = {}\n",
    "\n",
    "    for doc_id, chunk_dict in mapped_document_db.items():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "        #converting chunk embedding to numpy array for efficient mathematical operations\n",
    "            chunk_embeddings = np.array(chunk_embeddings)\n",
    "            score = calculate_cosine_similarity_score(query_embeddings, chunk_embeddings)\n",
    "            scores[(doc_id, chunk_id )] = score\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "\n",
    "    return sorted_scores\n"
   ],
   "id": "131746a03156b2f9",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.372194Z",
     "start_time": "2025-07-12T20:14:40.369542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_top_results(sorted_scores):\n",
    "    top_results=[]\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        results = (doc_id, chunk_id, score)\n",
    "        top_results.append(results)\n",
    "    return top_results\n"
   ],
   "id": "9c04ff42de7d1683",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.391626Z",
     "start_time": "2025-07-12T20:14:40.388254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_json(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n"
   ],
   "id": "4e2632c80dcff2bb",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.422819Z",
     "start_time": "2025-07-12T20:14:40.419309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_text(top_results, document_data):\n",
    "    first_match = top_results[0]\n",
    "    doc_id = first_match[0]\n",
    "    chunk_id = first_match[1]\n",
    "    related_text = document_data[doc_id][chunk_id]\n",
    "    return related_text\n"
   ],
   "id": "100f0ca4ffa44331",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:14:40.444676Z",
     "start_time": "2025-07-12T20:14:40.440538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_llm_response(openai_model, query, relevant_text):\n",
    "    template = \"\"\"\n",
    "    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n",
    "\n",
    "    Your job is to understand the request, and answer based on the retrieved context.\n",
    "    Here is context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "    chain = prompt | openai_model\n",
    "    response=chain.invoke({\"context\":relevant_text[\"text\"],\"question\":query})\n",
    "    return response\n"
   ],
   "id": "92165983c2d54a89",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:17:01.098362Z",
     "start_time": "2025-07-12T20:16:43.243300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"preprocessed_texts\"\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    chunk_size = 200\n",
    "    para_seperator=\" /n /n\"\n",
    "    separator=\" \"\n",
    "    top_k = 2\n",
    "    openai_model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "    #creating document store with chunk id, doc_id, text\n",
    "    documents = chunking(directory_path, tokenizer, chunk_size, para_seperator, separator)\n",
    "\n",
    "    #now embedding generation and mapping in database\n",
    "    mapped_document_db = map_document_embeddings(documents, tokenizer, model)\n",
    "\n",
    "    #saving json\n",
    "    save_json('database/doc_store_2.json', documents)\n",
    "    save_json('database/vector_store_2.json', mapped_document_db)\n",
    "\n",
    "    #Retrieving most relevant data chunks\n",
    "    query = \"How do I improve my VO2 max?\"\n",
    "    query_embeddings = compute_embeddings(query, tokenizer, model)\n",
    "    sorted_scores = retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k)\n",
    "    top_results = retrieve_top_results(sorted_scores)\n",
    "\n",
    "    #reading json\n",
    "    document_data = read_json(\"database/doc_store_2.json\") #read document store\n",
    "\n",
    "    #Retrieving text of relevant chunk embeddings\n",
    "    relevant_text = retrieve_text(top_results, document_data)\n",
    "\n",
    "    print(relevant_text)\n",
    "    #print(relevant_text[\"text\"])\n",
    "\n",
    "   #Uncomment if you have api key\n",
    "    #response = generate_llm_response(openai_model, query, relevant_text)\n",
    "    #print(response)"
   ],
   "id": "619d8ab4b4732fe3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vo2_1.txt\n",
      "vo2_2.txt\n",
      "vo2_3.txt\n",
      "{'text': 'real-\\nworld running activities of 14000 individuals with 1.6\\nmillion exercise sessions and a total distance of 20 million\\nkm found that faster runners partake in greater volumes of\\nlit than slower runners which was associated with better\\nperformance during high-intensity exercise 30.\\nresearch has shown that hit increases vo2max in\\nhealthy adults 24. scribbans et al. 22 found that hit\\n8092.5 vo2max was a powerful method for eliciting\\nimprovements in vo2max 0.26  0.10 l.min1 es  0.68.\\nweston et al. 14 included a cohort of both healthy and\\nsedentary participants reporting moderate improvements in\\nvo2max for both active nonathletic 6.2  3.1 and sed-\\nentary men 10  5.1 as well as active nonathletic\\n3.6  4.3 and sedentary women 7.3  4.8 when\\ncompared to a control group 1.2  2.0. wen et', 'metadata': {'file_name': 'vo2_3'}}\n"
     ]
    }
   ],
   "execution_count": 76
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
