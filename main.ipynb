{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.282875Z",
     "start_time": "2025-07-12T19:06:54.278975Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.330166Z",
     "start_time": "2025-07-12T19:06:54.323328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import uuid\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ],
   "id": "20671ee778690f7e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.351317Z",
     "start_time": "2025-07-12T19:06:54.347383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#YOUR KEY HERE\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ],
   "id": "76ea39dac6f29c0e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.373951Z",
     "start_time": "2025-07-12T19:06:54.366611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunking(directory_path, tokenizer, chunk_size, para_seperator=\" /n /n\", separator=\" \"):\n",
    "\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    documents = {}\n",
    "    all_chunks = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        print(filename)\n",
    "        base = os.path.basename(file_path)\n",
    "        sku = os.path.splitext(base)[0]\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            doc_id = str(uuid.uuid4())\n",
    "\n",
    "            paragraphs = re.split(para_seperator, text)\n",
    "\n",
    "            for paragraph in paragraphs:\n",
    "                words = paragraph.split(separator)\n",
    "                current_chunk_str = \"\"\n",
    "                chunk = []\n",
    "                for word in words:\n",
    "                    if current_chunk_str:\n",
    "                        new_chunk = current_chunk_str + separator + word\n",
    "                    else:\n",
    "                        new_chunk = current_chunk_str + word\n",
    "                    if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n",
    "                        current_chunk_str = new_chunk\n",
    "                    else:\n",
    "                        if current_chunk_str:\n",
    "                            chunk.append(current_chunk_str)\n",
    "                        current_chunk_str = word\n",
    "\n",
    "\n",
    "                if current_chunk_str:\n",
    "                    chunk.append(current_chunk_str)\n",
    "\n",
    "                for chunk in chunk:\n",
    "                    chunk_id = str(uuid.uuid4())\n",
    "                    all_chunks[chunk_id] = {\"text\": chunk, \"metadata\": {\"file_name\":sku}}\n",
    "        documents[doc_id] = all_chunks\n",
    "    return documents\n"
   ],
   "id": "1a1ba973421970c2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.387281Z",
     "start_time": "2025-07-12T19:06:54.382381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def map_document_embeddings(documents, tokenizer, model):\n",
    "    mapped_document_db = {}\n",
    "    for id, dict_content in documents.items():\n",
    "        mapped_embeddings = {}\n",
    "        for content_id, text_content in dict_content.items():\n",
    "            text = text_content.get(\"text\")\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "            mapped_embeddings[content_id] = embeddings\n",
    "        mapped_document_db[id] = mapped_embeddings\n",
    "    return mapped_document_db\n"
   ],
   "id": "376a459c311b6656",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.414532Z",
     "start_time": "2025-07-12T19:06:54.406702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_information(query, top_k, mapped_document_db):\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    query_embeddings = model(**query_inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "    query_embeddings=query_embeddings.tolist()\n",
    "    #converting query embeddings to numpy array\n",
    "    query_embeddings=np.array(query_embeddings)\n",
    "\n",
    "    scores = {}\n",
    "    #Now calculating cosine similarity\n",
    "    for doc_id, chunk_dict in mapped_document_db.items():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "            #converting chunk embedding to numpy array for efficient mathematical operations\n",
    "            chunk_embeddings = np.array(chunk_embeddings)\n",
    "\n",
    "            #Normalizing chunk embeddings and query embeddings  to get cosine similarity score\n",
    "            normalized_query = np.linalg.norm(query_embeddings)\n",
    "            normalized_chunk = np.linalg.norm(chunk_embeddings)\n",
    "\n",
    "            if normalized_chunk == 0 or normalized_query == 0:\n",
    "            # this is being done to avoid division with zero which will give wrong results i.e infinity. Hence to avoid this we set score to 0\n",
    "                score == 0\n",
    "            else:\n",
    "            # Now calculating cosine similarity score\n",
    "                score = np.dot(chunk_embeddings, query_embeddings)/ (normalized_chunk * normalized_query)\n",
    "\n",
    "             #STORING SCORES WITH THE REFERENCE\n",
    "            scores[(doc_id, chunk_id )] = score\n",
    "\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "\n",
    "    top_results=[]\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        results = (doc_id, chunk_id, score)\n",
    "        top_results.append(results)\n",
    "    return top_results\n"
   ],
   "id": "78d9890b7f9b19ec",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.433492Z",
     "start_time": "2025-07-12T19:06:54.429504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_embeddings(query, tokenizer, model):\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    query_embeddings = model(**query_inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "    query_embeddings=query_embeddings.tolist()\n",
    "    return query_embeddings\n"
   ],
   "id": "eb32ac0277ef8d02",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.453268Z",
     "start_time": "2025-07-12T19:06:54.448446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_cosine_similarity_score(query_embeddings, chunk_embeddings):\n",
    "        normalized_query = np.linalg.norm(query_embeddings)\n",
    "        normalized_chunk = np.linalg.norm(chunk_embeddings)\n",
    "        if normalized_chunk == 0 or normalized_query == 0:\n",
    "            score == 0\n",
    "        else:\n",
    "            score = np.dot(chunk_embeddings, query_embeddings)/ (normalized_chunk * normalized_query)\n",
    "        return score\n",
    "\n"
   ],
   "id": "3cb9eb1ad23627b2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.481203Z",
     "start_time": "2025-07-12T19:06:54.476474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k):\n",
    "    scores = {}\n",
    "\n",
    "    for doc_id, chunk_dict in mapped_document_db.items():\n",
    "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
    "        #converting chunk embedding to numpy array for efficient mathematical operations\n",
    "            chunk_embeddings = np.array(chunk_embeddings)\n",
    "            score = calculate_cosine_similarity_score(query_embeddings, chunk_embeddings)\n",
    "            scores[(doc_id, chunk_id )] = score\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "\n",
    "    return sorted_scores\n"
   ],
   "id": "131746a03156b2f9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.500885Z",
     "start_time": "2025-07-12T19:06:54.496192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_top_results(sorted_scores):\n",
    "    top_results=[]\n",
    "    for ((doc_id, chunk_id), score) in sorted_scores:\n",
    "        results = (doc_id, chunk_id, score)\n",
    "        top_results.append(results)\n",
    "    return top_results\n"
   ],
   "id": "9c04ff42de7d1683",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.519327Z",
     "start_time": "2025-07-12T19:06:54.513385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_json(path, data):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n"
   ],
   "id": "4e2632c80dcff2bb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.537023Z",
     "start_time": "2025-07-12T19:06:54.533305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_text(top_results, document_data):\n",
    "    first_match = top_results[0]\n",
    "    doc_id = first_match[0]\n",
    "    chunk_id = first_match[1]\n",
    "    related_text = document_data[doc_id][chunk_id]\n",
    "    return related_text\n"
   ],
   "id": "100f0ca4ffa44331",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:54.555008Z",
     "start_time": "2025-07-12T19:06:54.550877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_llm_response(openai_model, query, relevant_text):\n",
    "    template = \"\"\"\n",
    "    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n",
    "\n",
    "    Your job is to understand the request, and answer based on the retrieved context.\n",
    "    Here is context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "    chain = prompt | openai_model\n",
    "    response=chain.invoke({\"context\":relevant_text[\"text\"],\"question\":query})\n",
    "    return response\n",
    "\n",
    "\n"
   ],
   "id": "92165983c2d54a89",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T19:06:57.269373Z",
     "start_time": "2025-07-12T19:06:54.566510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"preprocessed_texts\"\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    chunk_size = 200\n",
    "    para_seperator=\" /n /n\"\n",
    "    separator=\" \"\n",
    "    top_k = 2\n",
    "    openai_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "    #creating document store with chunk id, doc_id, text\n",
    "    documents = chunking(directory_path, tokenizer, chunk_size, para_seperator, separator)\n",
    "\n",
    "    #now embedding generation and mapping in database\n",
    "    mapped_document_db = map_document_embeddings(documents, tokenizer, model)\n",
    "\n",
    "    #saving json\n",
    "    save_json('database/doc_store_2.json', documents)\n",
    "    save_json('database/vector_store_2.json', mapped_document_db)\n",
    "\n",
    "    #Retrieving most relevant data chunks\n",
    "    query = \"How do I improve my VO2 max?\"\n",
    "    query_embeddings = compute_embeddings(query, tokenizer, model)\n",
    "    sorted_scores = retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k)\n",
    "    top_results = retrieve_top_results(sorted_scores)\n",
    "\n",
    "    #reading json\n",
    "    document_data = read_json(\"database/doc_store_2.json\") #read document store\n",
    "\n",
    "    #Retrieving text of relevant chunk embeddings\n",
    "    relevant_text = retrieve_text(top_results, document_data)\n",
    "\n",
    "    print(relevant_text)\n",
    "    #print(relevant_text[\"text\"])\n",
    "\n",
    "   #Uncomment if you have api key\n",
    "    response = generate_llm_response(openai_model, query, relevant_text)\n",
    "    print(response)"
   ],
   "id": "619d8ab4b4732fe3",
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOpenAIError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m separator=\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      9\u001B[39m top_k = \u001B[32m2\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m openai_model = \u001B[43mChatOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgpt-3.5-turbo\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m#creating document store with chunk id, doc_id, text\u001B[39;00m\n\u001B[32m     14\u001B[39m documents = chunking(directory_path, tokenizer, chunk_size, para_seperator, separator)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001B[39m, in \u001B[36mSerializable.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args: Any, **kwargs: Any) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    129\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa: D419\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[31m[... skipping hidden 1 frame]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:743\u001B[39m, in \u001B[36mBaseChatOpenAI.validate_environment\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    736\u001B[39m         \u001B[38;5;28mself\u001B[39m.http_client = httpx.Client(\n\u001B[32m    737\u001B[39m             proxy=\u001B[38;5;28mself\u001B[39m.openai_proxy, verify=global_ssl_context\n\u001B[32m    738\u001B[39m         )\n\u001B[32m    739\u001B[39m     sync_specific = {\n\u001B[32m    740\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mhttp_client\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m.http_client\n\u001B[32m    741\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _get_default_httpx_client(\u001B[38;5;28mself\u001B[39m.openai_api_base, \u001B[38;5;28mself\u001B[39m.request_timeout)\n\u001B[32m    742\u001B[39m     }\n\u001B[32m--> \u001B[39m\u001B[32m743\u001B[39m     \u001B[38;5;28mself\u001B[39m.root_client = \u001B[43mopenai\u001B[49m\u001B[43m.\u001B[49m\u001B[43mOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mclient_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43msync_specific\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m    744\u001B[39m     \u001B[38;5;28mself\u001B[39m.client = \u001B[38;5;28mself\u001B[39m.root_client.chat.completions\n\u001B[32m    745\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.async_client:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_client.py:130\u001B[39m, in \u001B[36mOpenAI.__init__\u001B[39m\u001B[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001B[39m\n\u001B[32m    128\u001B[39m     api_key = os.environ.get(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_API_KEY\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m api_key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m OpenAIError(\n\u001B[32m    131\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    132\u001B[39m     )\n\u001B[32m    133\u001B[39m \u001B[38;5;28mself\u001B[39m.api_key = api_key\n\u001B[32m    135\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m organization \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mOpenAIError\u001B[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
