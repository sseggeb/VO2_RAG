{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-12T17:02:57.436453Z",
     "start_time": "2025-07-12T17:02:42.327268Z"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssegg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ssegg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T18:19:03.049285Z",
     "start_time": "2025-07-12T18:19:02.898797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fitz #PyMuPDF\n",
    "import os"
   ],
   "id": "61b7c9d2f60069e6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T18:19:05.032285Z",
     "start_time": "2025-07-12T18:19:05.017205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_pdf(pdf_path, output_dir=\"extracted_texts\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        document = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(document.page_count):\n",
    "            page = document.load_page(page_num)\n",
    "            text += page.get_text() + \"\\n\" # add new line to separate text from different pages\n",
    "        document.close()\n",
    "\n",
    "        # create a clean filename for the text file\n",
    "        base_filename = os.path.basename(pdf_path)\n",
    "        text_filename = os.path.splitext(base_filename)[0] + \".txt\"\n",
    "        output_path = os.path.join(output_dir, text_filename)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Successfully extracted text from {pdf_path} to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path} to {output_path}: {e}\")\n"
   ],
   "id": "264fb18e972f4c66",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T18:19:09.992230Z",
     "start_time": "2025-07-12T18:19:09.980160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_all_pdfs_in_directory(pdf_dir, output_dir=\"extracted_texts\"):\n",
    "\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            extract_text_from_pdf(pdf_path, output_dir)"
   ],
   "id": "6699e6ffb6aeadee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T17:15:51.201506Z",
     "start_time": "2025-07-12T17:15:50.680894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pdf_input_directory = \"pdf_papers\"\n",
    "text_output_directory = \"extracted_texts\"\n",
    "\n",
    "process_all_pdfs_in_directory(pdf_input_directory, text_output_directory)"
   ],
   "id": "c2fd750ad02532bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted text from pdf_papers\\40279_2024_Article_2067.pdf to extracted_texts\\40279_2024_Article_2067.txt\n",
      "Successfully extracted text from pdf_papers\\40798_2025_Article_848.pdf to extracted_texts\\40798_2025_Article_848.txt\n",
      "Successfully extracted text from pdf_papers\\EJSC-24-1779.pdf to extracted_texts\\EJSC-24-1779.txt\n",
      "Successfully extracted text from pdf_papers\\fphys-10-00363.pdf to extracted_texts\\fphys-10-00363.txt\n",
      "Successfully extracted text from pdf_papers\\fphys-10-00375.pdf to extracted_texts\\fphys-10-00375.txt\n",
      "Successfully extracted text from pdf_papers\\fphys-13-835705.pdf to extracted_texts\\fphys-13-835705.txt\n",
      "Successfully extracted text from pdf_papers\\fspor-05-1147475.pdf to extracted_texts\\fspor-05-1147475.txt\n",
      "Successfully extracted text from pdf_papers\\nutrients-16-00571.pdf to extracted_texts\\nutrients-16-00571.txt\n",
      "Successfully extracted text from pdf_papers\\pone.0143028.pdf to extracted_texts\\pone.0143028.txt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing of the extracted text files",
   "id": "e7b1dca6364d602a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T18:19:27.336213Z",
     "start_time": "2025-07-12T18:19:14.737313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ],
   "id": "56fc3e7f080fcfdc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T18:27:21.000021Z",
     "start_time": "2025-07-12T18:27:20.991115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set of english stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_and_preprocess(text):\n",
    "    # remove URLs and emails\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "    # remove short lines\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if len(line.strip()) > 10 or (len(line.strip()) > 0 and not line.strip().isdigit()):\n",
    "            cleaned_lines.append(line)\n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation except for some essential ones for sentence structure like periods\n",
    "    # keeps letters, numbers, and common sentence terminators (. ! ?)\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.\\!\\?]', '', text)\n",
    "\n",
    "    # re-join periods (etc) to sentences after cleanup if they were separated\n",
    "    # better tokenization later\n",
    "    text = text.replace(' .', '.').replace(' ?','?').replace(' !','!')\n",
    "\n",
    "    # other pdf cleaning depending on the actual files I have here...\n",
    "    # headers, footers, page numbers, reference sections?\n",
    "    text = re.split(r'(?i)References|Bibliography|Acknowledgements|Appendix', text)[0]\n",
    "    return text"
   ],
   "id": "fc82add0238f0579",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T18:27:24.772111Z",
     "start_time": "2025-07-12T18:27:24.764234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_all_text_files(input_dir, output_dir=\"preprocessed_texts\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.lower().endswith(\".txt\"):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            try:\n",
    "                with open(input_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "                    raw_text = f_in.read()\n",
    "\n",
    "                cleaned_text = clean_and_preprocess(raw_text)\n",
    "\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                    f_out.write(cleaned_text)\n",
    "                print(f\"Successfully preprocessed '{input_path}' to '{output_path}'\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error preprocessing '{input_path}' to '{output_path}': {e}\")"
   ],
   "id": "23904219e890e741",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T18:27:27.934177Z",
     "start_time": "2025-07-12T18:27:27.838101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extracted_text_dictionary = \"extracted_texts\"\n",
    "preprocessed_text_dictionary = \"preprocessed_texts\"\n",
    "\n",
    "preprocess_all_text_files(extracted_text_dictionary, preprocessed_text_dictionary)"
   ],
   "id": "f6e500f277f17d7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preprocessed 'extracted_texts\\40279_2024_Article_2067.txt' to 'preprocessed_texts\\40279_2024_Article_2067.txt'\n",
      "Successfully preprocessed 'extracted_texts\\40798_2025_Article_848.txt' to 'preprocessed_texts\\40798_2025_Article_848.txt'\n",
      "Successfully preprocessed 'extracted_texts\\EJSC-24-1779.txt' to 'preprocessed_texts\\EJSC-24-1779.txt'\n",
      "Successfully preprocessed 'extracted_texts\\fphys-10-00363.txt' to 'preprocessed_texts\\fphys-10-00363.txt'\n",
      "Successfully preprocessed 'extracted_texts\\fphys-10-00375.txt' to 'preprocessed_texts\\fphys-10-00375.txt'\n",
      "Successfully preprocessed 'extracted_texts\\fphys-13-835705.txt' to 'preprocessed_texts\\fphys-13-835705.txt'\n",
      "Successfully preprocessed 'extracted_texts\\fspor-05-1147475.txt' to 'preprocessed_texts\\fspor-05-1147475.txt'\n",
      "Successfully preprocessed 'extracted_texts\\nutrients-16-00571.txt' to 'preprocessed_texts\\nutrients-16-00571.txt'\n",
      "Successfully preprocessed 'extracted_texts\\pone.0143028.txt' to 'preprocessed_texts\\pone.0143028.txt'\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
