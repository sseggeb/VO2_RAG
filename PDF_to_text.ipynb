{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-12T20:57:03.045258Z",
     "start_time": "2025-07-12T20:57:03.027383Z"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssegg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ssegg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ssegg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:57:03.115939Z",
     "start_time": "2025-07-12T20:57:03.110969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fitz #PyMuPDF\n",
    "import os"
   ],
   "id": "61b7c9d2f60069e6",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:57:03.162589Z",
     "start_time": "2025-07-12T20:57:03.155916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_pdf(pdf_path, output_dir=\"extracted_texts\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        document = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(document.page_count):\n",
    "            page = document.load_page(page_num)\n",
    "            text += page.get_text() + \"\\n\" # add new line to separate text from different pages\n",
    "        document.close()\n",
    "\n",
    "        # create a clean filename for the text file\n",
    "        base_filename = os.path.basename(pdf_path)\n",
    "        text_filename = os.path.splitext(base_filename)[0] + \".txt\"\n",
    "        output_path = os.path.join(output_dir, text_filename)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Successfully extracted text from {pdf_path} to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path} to {output_path}: {e}\")\n"
   ],
   "id": "264fb18e972f4c66",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:57:03.184698Z",
     "start_time": "2025-07-12T20:57:03.179375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_all_pdfs_in_directory(pdf_dir, output_dir=\"extracted_texts\"):\n",
    "\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            extract_text_from_pdf(pdf_path, output_dir)"
   ],
   "id": "6699e6ffb6aeadee",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:57:03.362236Z",
     "start_time": "2025-07-12T20:57:03.203198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pdf_input_directory = \"pdf_papers\"\n",
    "text_output_directory = \"extracted_texts\"\n",
    "\n",
    "process_all_pdfs_in_directory(pdf_input_directory, text_output_directory)"
   ],
   "id": "c2fd750ad02532bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted text from pdf_papers\\vo2_1.pdf to extracted_texts\\vo2_1.txt\n",
      "Successfully extracted text from pdf_papers\\vo2_2.pdf to extracted_texts\\vo2_2.txt\n",
      "Successfully extracted text from pdf_papers\\vo2_3.pdf to extracted_texts\\vo2_3.txt\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing of the extracted text files",
   "id": "e7b1dca6364d602a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:57:03.381807Z",
     "start_time": "2025-07-12T20:57:03.377241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ],
   "id": "56fc3e7f080fcfdc",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:57:03.408004Z",
     "start_time": "2025-07-12T20:57:03.399573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set of english stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_and_preprocess(text):\n",
    "    # remove URLs and emails\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # 5. Remove extra whitespace, tabs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # remove punctuation except for some essential ones for sentence structure like periods\n",
    "    # keeps letters, numbers, and common sentence terminators (. ! ? -)\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.\\!\\?\\-]', '', text)\n",
    "\n",
    "    # remove short lines\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if len(line.strip()) > 10 or (len(line.strip()) > 0 and not line.strip().isdigit()):\n",
    "            cleaned_lines.append(line)\n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "\n",
    "    # other pdf cleaning depending on the actual files I have here... there is quite a bit to be done to clean the pdfs up more\n",
    "    # headers, footers, page numbers, reference sections?\n",
    "    text = re.split(r'(?i)References|Bibliography|Acknowledgements|Appendix', text)[0]\n",
    "\n",
    "    # remove stop words (test with and without) (the, a, is, and, etc)\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in STOP_WORDS]\n",
    "    text = ' '.join(filtered_words)\n",
    "\n",
    "    # re-join periods (etc) to sentences after cleanup if they were separated\n",
    "    # better tokenization later\n",
    "    text = text.replace(' .', '.').replace(' ?','?').replace(' !','!')\n",
    "\n",
    "    return text"
   ],
   "id": "fc82add0238f0579",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:57:03.427086Z",
     "start_time": "2025-07-12T20:57:03.421499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_all_text_files(input_dir, output_dir=\"preprocessed_texts\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.lower().endswith(\".txt\"):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            try:\n",
    "                with open(input_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "                    raw_text = f_in.read()\n",
    "\n",
    "                cleaned_text = clean_and_preprocess(raw_text)\n",
    "\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                    f_out.write(cleaned_text)\n",
    "                print(f\"Successfully preprocessed '{input_path}' to '{output_path}'\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error preprocessing '{input_path}' to '{output_path}': {e}\")"
   ],
   "id": "23904219e890e741",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T20:57:03.523177Z",
     "start_time": "2025-07-12T20:57:03.441781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extracted_text_dictionary = \"extracted_texts\"\n",
    "preprocessed_text_dictionary = \"preprocessed_texts\"\n",
    "\n",
    "preprocess_all_text_files(extracted_text_dictionary, preprocessed_text_dictionary)"
   ],
   "id": "f6e500f277f17d7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preprocessed 'extracted_texts\\vo2_1.txt' to 'preprocessed_texts\\vo2_1.txt'\n",
      "Successfully preprocessed 'extracted_texts\\vo2_2.txt' to 'preprocessed_texts\\vo2_2.txt'\n",
      "Successfully preprocessed 'extracted_texts\\vo2_3.txt' to 'preprocessed_texts\\vo2_3.txt'\n"
     ]
    }
   ],
   "execution_count": 88
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
